{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, LSTM, Dropout, Flatten, Dense, Conv1D, Reshape, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam \n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import clone_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r\"C:\\Users\\Yash Vikram Singh\\Downloads\\emotions.csv\\emotions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ascii\n"
     ]
    }
   ],
   "source": [
    "import chardet\n",
    "\n",
    "with open(r\"C:\\Users\\Yash Vikram Singh\\Downloads\\emotions.csv\\emotions.csv\", \"rb\") as file:\n",
    "    result = chardet.detect(file.read())\n",
    "    print(result[\"encoding\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import kagglehub\n",
    "# kagglehub.dataset_download('birdy654/eeg-brainwave-dataset-feeling-emotions', path='emotions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['# mean_0_a', 'mean_1_a', 'mean_2_a', 'mean_3_a', 'mean_4_a',\n",
      "       'mean_d_0_a', 'mean_d_1_a', 'mean_d_2_a', 'mean_d_3_a', 'mean_d_4_a',\n",
      "       ...\n",
      "       'fft_741_b', 'fft_742_b', 'fft_743_b', 'fft_744_b', 'fft_745_b',\n",
      "       'fft_746_b', 'fft_747_b', 'fft_748_b', 'fft_749_b', 'label'],\n",
      "      dtype='object', length=2549)\n"
     ]
    }
   ],
   "source": [
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mappping = {'NEGATIVE':0, 'NEUTRAL':1, 'POSITIVE':2}\n",
    "data['label'] = data['label'].map(label_mappping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># mean_0_a</th>\n",
       "      <th>mean_1_a</th>\n",
       "      <th>mean_2_a</th>\n",
       "      <th>mean_3_a</th>\n",
       "      <th>mean_4_a</th>\n",
       "      <th>mean_d_0_a</th>\n",
       "      <th>mean_d_1_a</th>\n",
       "      <th>mean_d_2_a</th>\n",
       "      <th>mean_d_3_a</th>\n",
       "      <th>mean_d_4_a</th>\n",
       "      <th>...</th>\n",
       "      <th>fft_741_b</th>\n",
       "      <th>fft_742_b</th>\n",
       "      <th>fft_743_b</th>\n",
       "      <th>fft_744_b</th>\n",
       "      <th>fft_745_b</th>\n",
       "      <th>fft_746_b</th>\n",
       "      <th>fft_747_b</th>\n",
       "      <th>fft_748_b</th>\n",
       "      <th>fft_749_b</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.62</td>\n",
       "      <td>30.3</td>\n",
       "      <td>-356.0</td>\n",
       "      <td>15.6</td>\n",
       "      <td>26.3</td>\n",
       "      <td>1.070</td>\n",
       "      <td>0.411</td>\n",
       "      <td>-15.70</td>\n",
       "      <td>2.06</td>\n",
       "      <td>3.15</td>\n",
       "      <td>...</td>\n",
       "      <td>23.5</td>\n",
       "      <td>20.3</td>\n",
       "      <td>20.3</td>\n",
       "      <td>23.5</td>\n",
       "      <td>-215.0</td>\n",
       "      <td>280.00</td>\n",
       "      <td>-162.00</td>\n",
       "      <td>-162.00</td>\n",
       "      <td>280.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28.80</td>\n",
       "      <td>33.1</td>\n",
       "      <td>32.0</td>\n",
       "      <td>25.8</td>\n",
       "      <td>22.8</td>\n",
       "      <td>6.550</td>\n",
       "      <td>1.680</td>\n",
       "      <td>2.88</td>\n",
       "      <td>3.83</td>\n",
       "      <td>-4.82</td>\n",
       "      <td>...</td>\n",
       "      <td>-23.3</td>\n",
       "      <td>-21.8</td>\n",
       "      <td>-21.8</td>\n",
       "      <td>-23.3</td>\n",
       "      <td>182.0</td>\n",
       "      <td>2.57</td>\n",
       "      <td>-31.60</td>\n",
       "      <td>-31.60</td>\n",
       "      <td>2.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.90</td>\n",
       "      <td>29.4</td>\n",
       "      <td>-416.0</td>\n",
       "      <td>16.7</td>\n",
       "      <td>23.7</td>\n",
       "      <td>79.900</td>\n",
       "      <td>3.360</td>\n",
       "      <td>90.20</td>\n",
       "      <td>89.90</td>\n",
       "      <td>2.03</td>\n",
       "      <td>...</td>\n",
       "      <td>462.0</td>\n",
       "      <td>-233.0</td>\n",
       "      <td>-233.0</td>\n",
       "      <td>462.0</td>\n",
       "      <td>-267.0</td>\n",
       "      <td>281.00</td>\n",
       "      <td>-148.00</td>\n",
       "      <td>-148.00</td>\n",
       "      <td>281.00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.90</td>\n",
       "      <td>31.6</td>\n",
       "      <td>-143.0</td>\n",
       "      <td>19.8</td>\n",
       "      <td>24.3</td>\n",
       "      <td>-0.584</td>\n",
       "      <td>-0.284</td>\n",
       "      <td>8.82</td>\n",
       "      <td>2.30</td>\n",
       "      <td>-1.97</td>\n",
       "      <td>...</td>\n",
       "      <td>299.0</td>\n",
       "      <td>-243.0</td>\n",
       "      <td>-243.0</td>\n",
       "      <td>299.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>-12.40</td>\n",
       "      <td>9.53</td>\n",
       "      <td>9.53</td>\n",
       "      <td>-12.40</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28.30</td>\n",
       "      <td>31.3</td>\n",
       "      <td>45.2</td>\n",
       "      <td>27.3</td>\n",
       "      <td>24.5</td>\n",
       "      <td>34.800</td>\n",
       "      <td>-5.790</td>\n",
       "      <td>3.06</td>\n",
       "      <td>41.40</td>\n",
       "      <td>5.52</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>38.1</td>\n",
       "      <td>38.1</td>\n",
       "      <td>12.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>-17.60</td>\n",
       "      <td>23.90</td>\n",
       "      <td>23.90</td>\n",
       "      <td>-17.60</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 2549 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   # mean_0_a  mean_1_a  mean_2_a  mean_3_a  mean_4_a  mean_d_0_a  mean_d_1_a  \\\n",
       "0        4.62      30.3    -356.0      15.6      26.3       1.070       0.411   \n",
       "1       28.80      33.1      32.0      25.8      22.8       6.550       1.680   \n",
       "2        8.90      29.4    -416.0      16.7      23.7      79.900       3.360   \n",
       "3       14.90      31.6    -143.0      19.8      24.3      -0.584      -0.284   \n",
       "4       28.30      31.3      45.2      27.3      24.5      34.800      -5.790   \n",
       "\n",
       "   mean_d_2_a  mean_d_3_a  mean_d_4_a  ...  fft_741_b  fft_742_b  fft_743_b  \\\n",
       "0      -15.70        2.06        3.15  ...       23.5       20.3       20.3   \n",
       "1        2.88        3.83       -4.82  ...      -23.3      -21.8      -21.8   \n",
       "2       90.20       89.90        2.03  ...      462.0     -233.0     -233.0   \n",
       "3        8.82        2.30       -1.97  ...      299.0     -243.0     -243.0   \n",
       "4        3.06       41.40        5.52  ...       12.0       38.1       38.1   \n",
       "\n",
       "   fft_744_b  fft_745_b  fft_746_b  fft_747_b  fft_748_b  fft_749_b  label  \n",
       "0       23.5     -215.0     280.00    -162.00    -162.00     280.00      0  \n",
       "1      -23.3      182.0       2.57     -31.60     -31.60       2.57      1  \n",
       "2      462.0     -267.0     281.00    -148.00    -148.00     281.00      2  \n",
       "3      299.0      132.0     -12.40       9.53       9.53     -12.40      2  \n",
       "4       12.0      119.0     -17.60      23.90      23.90     -17.60      1  \n",
       "\n",
       "[5 rows x 2549 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:, :-1].values\n",
    "y = pd.get_dummies(data['label']).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, Y_train, y_val = train_test_split(X, y, test_size = 0.2, random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lstm = np.expand_dims(X_train, axis=-1)\n",
    "X_val_lstm = np.expand_dims(X_val, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_lstm = Input(shape=(X_train.shape[1], 1))\n",
    "x_lstm = LSTM(256, return_sequences=True)(i_lstm)\n",
    "x_lstm = Dropout (0.6)(x_lstm)\n",
    "x_lstm = Flatten()(x_lstm)\n",
    "y_lstm = Dense(3, activation='softmax')(x_lstm)\n",
    "model_lstm = Model(i_lstm, y_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_lstm = Adam(learning_rate=0.000001)\n",
    "es = EarlyStopping(monitor = 'val_accuracy', mode = 'max', patience = 20, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm.compile(optimizer=adam_lstm,loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 497s 122s/step - loss: 1.0983 - accuracy: 0.4258 - val_loss: 0.9144 - val_accuracy: 0.6557\n"
     ]
    }
   ],
   "source": [
    "lstm_h = model_lstm.fit(X_train_lstm, Y_train,\n",
    "                        batch_size=512,\n",
    "                        validation_data=(X_val_lstm, y_val),\n",
    "                        epochs=1,\n",
    "                        callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape_capsule = (X_train.shape[1], 1)\n",
    "input_layer_capsule = Input(shape=input_shape_capsule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1_params={\n",
    "    \"filters\": 256,\n",
    "    \"kernel_size\": 11,\n",
    "    \"strides\": 1,\n",
    "    \"padding\":\"valid\",\n",
    "    \"activation\": \"softmax\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_caps = Conv1D(**conv1_params)(input_layer_capsule)\n",
    "primary_caps_reshaped = Reshape((-1, 256))(primary_caps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_capsule = 5\n",
    "dim_capsule = 16\n",
    "capsule_layer = Dense(num_capsule * dim_capsule, activation = \"relu\")(primary_caps_reshaped)\n",
    "capsule_layer_flattened = Flatten()(capsule_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squash(x, axis=-1):\n",
    "    squared_norm = tf.reduce_sum(tf.square(x), axis, keepdims=True)\n",
    "    scale = squared_norm / (1+squared_norm)\n",
    "    return scale*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "capsule_layer_squashed = Lambda(squash)(capsule_layer_flattened)\n",
    "output_layer_capsule = Dense(3, activation='softmax')(capsule_layer_squashed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_capsule = Model(inputs=input_layer_capsule, outputs=output_layer_capsule)\n",
    "model_capsule.compile(optimizer=\"adam\",\n",
    "                    loss=\"categorical_crossentropy\",\n",
    "                    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Particle Swarn Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_particles = 5\n",
    "max_iterations = 5\n",
    "w = 0.5  \n",
    "c1 = 1.5  \n",
    "c2 = 1.5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/1 - Best Fitness: 129.66722106933594\n",
      "14/14 [==============================] - 12s 749ms/step - loss: 94.9481 - accuracy: 0.4426\n",
      "Optimized LSTM - Loss: 94.94805908203125, Accuracy: 0.44262295961380005\n",
      "Iteration 1/1 - Best Fitness: 91.5803451538086\n",
      "14/14 [==============================] - 2s 86ms/step - loss: 77.5502 - accuracy: 0.3372\n",
      "Optimized Capsule Network - Loss: 77.55021667480469, Accuracy: 0.33723652362823486\n"
     ]
    }
   ],
   "source": [
    "def flatten_weights(model):\n",
    "    \"\"\"Flatten the weights of a Keras model into a 1D array.\"\"\"\n",
    "    flattened_weights = []\n",
    "    for layer in model.layers:\n",
    "        layer_weights = layer.get_weights()\n",
    "        if layer_weights:\n",
    "            flattened_weights.extend([w.flatten() for w in layer_weights])\n",
    "    return np.concatenate(flattened_weights)\n",
    "\n",
    "def reconstruct_weights(flattened_weights, model):\n",
    "    start_idx = 0\n",
    "    for layer in model.layers:\n",
    "        layer_weights = layer.get_weights()\n",
    "        if layer_weights: \n",
    "            weight_shapes = [w.shape for w in layer_weights]\n",
    "            new_weights = [flattened_weights[start_idx:start_idx + np.prod(shape)].reshape(shape)\n",
    "                           for shape in weight_shapes]\n",
    "            layer.set_weights(new_weights)\n",
    "            start_idx += sum([np.prod(shape) for shape in weight_shapes])\n",
    "\n",
    "def initialize_particles_in_chunks(num_particles, weight_dim, chunk_size=100000):\n",
    "    \"\"\"Initialize particles and velocities in chunks to save memory.\"\"\"\n",
    "    particles = np.zeros((num_particles, weight_dim), dtype=np.float32)\n",
    "    velocities = np.zeros((num_particles, weight_dim), dtype=np.float32)\n",
    "    for start in range(0, weight_dim, chunk_size):\n",
    "        end = min(start + chunk_size, weight_dim)\n",
    "        particles[:, start:end] = np.random.uniform(-1, 1, (num_particles, end - start))\n",
    "        velocities[:, start:end] = np.random.uniform(-0.1, 0.1, (num_particles, end - start))\n",
    "    return particles, velocities\n",
    "\n",
    "def fitness_function(weights, model, X_val, y_val):\n",
    "    \"\"\"Evaluate the fitness of the weights using validation loss.\"\"\"\n",
    "    reconstruct_weights(weights, model)\n",
    "    loss, accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
    "    return loss \n",
    "\n",
    "def optimize_with_pso_reduced_memory(model, X_train, Y_train, X_val, y_val, num_particles=5, max_iterations=1, w=0.5, c1=1.5, c2=1.5):\n",
    "    model_clone = clone_model(model)\n",
    "    model_clone.set_weights(model.get_weights())\n",
    "    model_clone.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    initial_weights = flatten_weights(model_clone).astype(np.float32)\n",
    "    weight_dim = len(initial_weights)\n",
    "\n",
    "    particles, velocities = initialize_particles_in_chunks(num_particles, weight_dim)\n",
    "\n",
    "    pbest = particles.copy()\n",
    "    pbest_fitness = np.array([fitness_function(p, model_clone, X_val, y_val) for p in particles])\n",
    "    gbest = particles[np.argmin(pbest_fitness)]\n",
    "    gbest_fitness = min(pbest_fitness)\n",
    "\n",
    "    for iteration in range(max_iterations):\n",
    "        print(f\"Iteration {iteration + 1}/{max_iterations} - Best Fitness: {gbest_fitness}\")\n",
    "\n",
    "        for i in range(num_particles):\n",
    "            r1, r2 = np.random.rand(weight_dim).astype(np.float32), np.random.rand(weight_dim).astype(np.float32)\n",
    "            velocities[i] = (w * velocities[i] +\n",
    "                             c1 * r1 * (pbest[i] - particles[i]) +\n",
    "                             c2 * r2 * (gbest - particles[i]))\n",
    "\n",
    "            particles[i] += velocities[i]\n",
    "            fitness = fitness_function(particles[i], model_clone, X_val, y_val)\n",
    "\n",
    "            if fitness < pbest_fitness[i]:\n",
    "                pbest[i] = particles[i]\n",
    "                pbest_fitness[i] = fitness\n",
    "                if fitness < gbest_fitness:\n",
    "                    gbest = particles[i]\n",
    "                    gbest_fitness = fitness\n",
    "\n",
    "    reconstruct_weights(gbest, model)\n",
    "    return model\n",
    "\n",
    "optimized_lstm = optimize_with_pso_reduced_memory(model_lstm, X_train_lstm, Y_train, X_val_lstm, y_val)\n",
    "\n",
    "lstm_loss, lstm_accuracy = optimized_lstm.evaluate(X_val_lstm, y_val, verbose=1)\n",
    "print(f\"Optimized LSTM - Loss: {lstm_loss}, Accuracy: {lstm_accuracy}\")\n",
    "\n",
    "optimized_capsule = optimize_with_pso_reduced_memory(model_capsule, X_train_lstm, Y_train, X_val_lstm, y_val)\n",
    "\n",
    "capsule_loss, capsule_accuracy = optimized_capsule.evaluate(X_val_lstm, y_val)\n",
    "print(f\"Optimized Capsule Network - Loss: {capsule_loss}, Accuracy: {capsule_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ga-emotion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
